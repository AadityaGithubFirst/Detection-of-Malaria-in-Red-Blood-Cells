{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried different models. The last model was the one that I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x1a32d1b1010>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "  f = row['img_name']\n",
    "  l = row['label']\n",
    "  os.replace(f'train_images/{f}', f'train_images/{l}/{f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Load training data\n",
    "train_dir = 'train_images'\n",
    "train_image_paths_0 = [os.path.join(train_dir, '0', file) for file in os.listdir(os.path.join(train_dir, '0'))]\n",
    "train_image_paths_1 = [os.path.join(train_dir, '1', file) for file in os.listdir(os.path.join(train_dir, '1'))]\n",
    "train_image_paths = train_image_paths_0 + train_image_paths_1\n",
    "train_labels = [0]*len(train_image_paths_0) + [1]*len(train_image_paths_1)\n",
    "\n",
    "# Load test data\n",
    "test_dir = 'test_images'\n",
    "test_image_paths = [os.path.join(test_dir, file) for file in os.listdir(test_dir)]\n",
    "test_labels = [0]*len(test_image_paths)  # Dummy labels for test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Add data augmentation and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([  # For validation/test data\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 224x224 → 112x112\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 112x112 → 56x56\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # 56x56 → 28x28\n",
    "            \n",
    "            # Adaptive pooling to handle varying sizes\n",
    "            nn.AdaptiveAvgPool2d(1)  # 28x28 → 1x1\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Define data paths\n",
    "train_data_path = \"train_images\"\n",
    "test_data_path = \"test_images\"\n",
    "\n",
    "# Define transformations with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),        # Random crop to 224x224\n",
    "    transforms.RandomHorizontalFlip(),        # Random horizontal flip\n",
    "    transforms.RandomRotation(10),            # Random rotation ±10 degrees\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([     # For validation/test data\n",
    "    transforms.Resize(256),                   # Resize to 256x256\n",
    "    transforms.CenterCrop(224),               # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load full training dataset\n",
    "full_train_dataset = datasets.ImageFolder(\n",
    "    root=train_data_path,\n",
    "    transform=train_transform  # Initial transform for training data\n",
    ")\n",
    "\n",
    "# Split into train and validation (80-20 split)\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Override validation set transform to remove augmentation\n",
    "val_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class UnlabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [\n",
    "            os.path.join(root_dir, f) \n",
    "            for f in os.listdir(root_dir) \n",
    "            if os.path.isfile(os.path.join(root_dir, f))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image  # Returns only image, no label\n",
    "\n",
    "# Usage\n",
    "test_dataset = UnlabeledDataset(test_data_path, transform=val_test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).device)  # Should show 'cuda' or 'cpu'\n",
    "print(inputs.device)  # Should match model's device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)  # <-- Add .to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.36900084001430566, Val Loss: 0.25299355052951455, Val Acc: 0.9170068027210885\n",
      "Epoch 2, Train Loss: 0.2615396257192976, Val Loss: 0.2310144881569389, Val Acc: 0.9215419501133787\n",
      "Epoch 3, Train Loss: 0.2582839638588653, Val Loss: 0.2185787732132535, Val Acc: 0.919047619047619\n",
      "Epoch 4, Train Loss: 0.23782014914963773, Val Loss: 0.21214205199393674, Val Acc: 0.9317460317460318\n",
      "Epoch 5, Train Loss: 0.22805680138180437, Val Loss: 0.21865986911175045, Val Acc: 0.9321995464852608\n",
      "Epoch 6, Train Loss: 0.22297475352019502, Val Loss: 0.188397992618274, Val Acc: 0.9365079365079365\n",
      "Epoch 7, Train Loss: 0.22263558979983022, Val Loss: 0.1892873805448197, Val Acc: 0.9392290249433106\n",
      "Epoch 8, Train Loss: 0.2096282381064974, Val Loss: 0.18478156655918862, Val Acc: 0.9396825396825397\n",
      "Epoch 9, Train Loss: 0.20548889948649035, Val Loss: 0.18194430229672487, Val Acc: 0.9412698412698413\n",
      "Epoch 10, Train Loss: 0.19905544971824501, Val Loss: 0.18863429261398487, Val Acc: 0.9378684807256236\n",
      "Epoch 11, Train Loss: 0.2101060848791098, Val Loss: 0.18140681568479192, Val Acc: 0.9426303854875283\n",
      "Epoch 12, Train Loss: 0.2022549923259657, Val Loss: 0.1731872164738783, Val Acc: 0.9424036281179138\n",
      "Epoch 13, Train Loss: 0.19525960300360684, Val Loss: 0.17525642133061436, Val Acc: 0.9424036281179138\n",
      "Epoch 14, Train Loss: 0.1910287600307577, Val Loss: 0.1727372441386831, Val Acc: 0.9414965986394558\n",
      "Epoch 15, Train Loss: 0.18620812067546058, Val Loss: 0.17055874825387762, Val Acc: 0.9439909297052154\n",
      "Epoch 16, Train Loss: 0.18941808628944168, Val Loss: 0.17251430659730366, Val Acc: 0.946031746031746\n",
      "Epoch 17, Train Loss: 0.18135939240523113, Val Loss: 0.166786658672103, Val Acc: 0.9448979591836735\n",
      "Epoch 18, Train Loss: 0.1804593831597679, Val Loss: 0.1735543789126087, Val Acc: 0.9412698412698413\n",
      "Epoch 19, Train Loss: 0.18008281026318992, Val Loss: 0.1767181073301944, Val Acc: 0.9439909297052154\n",
      "Epoch 20, Train Loss: 0.19239616219588704, Val Loss: 0.17291331871588161, Val Acc: 0.9455782312925171\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "trigger_times = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)  # <-- Move to device\n",
    "        labels = labels.to(device)  # <-- Move to device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)  # <-- Move to device first!\n",
    "            labels = labels.to(device)  # <-- Move to device first!\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Rest of your epoch logging...\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}, Val Acc: {val_acc}')\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.6900391781330109\n",
      "Epoch 1, Batch 200, Loss: 0.6720368576049804\n",
      "Epoch 1, Batch 300, Loss: 0.6413507962226868\n",
      "Epoch 1, Batch 400, Loss: 0.6180286934971809\n",
      "Epoch 1, Batch 500, Loss: 0.615151093006134\n",
      "Epoch 1, Batch 600, Loss: 0.5814591497182846\n",
      "Epoch 2, Batch 100, Loss: 0.5642892575263977\n",
      "Epoch 2, Batch 200, Loss: 0.5462285140156746\n",
      "Epoch 2, Batch 300, Loss: 0.48377217918634413\n",
      "Epoch 2, Batch 400, Loss: 0.3915087629854679\n",
      "Epoch 2, Batch 500, Loss: 0.32680975273251534\n",
      "Epoch 2, Batch 600, Loss: 0.28059460908174516\n",
      "Epoch 3, Batch 100, Loss: 0.24788808040320873\n",
      "Epoch 3, Batch 200, Loss: 0.2328538653254509\n",
      "Epoch 3, Batch 300, Loss: 0.23175353705883026\n",
      "Epoch 3, Batch 400, Loss: 0.22736606072634458\n",
      "Epoch 3, Batch 500, Loss: 0.23295555870980025\n",
      "Epoch 3, Batch 600, Loss: 0.22020757231861354\n",
      "Epoch 4, Batch 100, Loss: 0.22321265410631896\n",
      "Epoch 4, Batch 200, Loss: 0.2046079248934984\n",
      "Epoch 4, Batch 300, Loss: 0.197715317979455\n",
      "Epoch 4, Batch 400, Loss: 0.1885259116999805\n",
      "Epoch 4, Batch 500, Loss: 0.19193736292421817\n",
      "Epoch 4, Batch 600, Loss: 0.20099692614749073\n",
      "Epoch 5, Batch 100, Loss: 0.18398837524466216\n",
      "Epoch 5, Batch 200, Loss: 0.18677538871765137\n",
      "Epoch 5, Batch 300, Loss: 0.19532745068892837\n",
      "Epoch 5, Batch 400, Loss: 0.1981575084477663\n",
      "Epoch 5, Batch 500, Loss: 0.22168048171326518\n",
      "Epoch 5, Batch 600, Loss: 0.1929979770630598\n",
      "Epoch 6, Batch 100, Loss: 0.17125961922109126\n",
      "Epoch 6, Batch 200, Loss: 0.17196284560486674\n",
      "Epoch 6, Batch 300, Loss: 0.18497245030477644\n",
      "Epoch 6, Batch 400, Loss: 0.17662835227325557\n",
      "Epoch 6, Batch 500, Loss: 0.1657872001081705\n",
      "Epoch 6, Batch 600, Loss: 0.1603454924374819\n",
      "Epoch 7, Batch 100, Loss: 0.13763746811077\n",
      "Epoch 7, Batch 200, Loss: 0.14820472870022058\n",
      "Epoch 7, Batch 300, Loss: 0.1432488564029336\n",
      "Epoch 7, Batch 400, Loss: 0.15275684842839837\n",
      "Epoch 7, Batch 500, Loss: 0.16284878876060246\n",
      "Epoch 7, Batch 600, Loss: 0.14662043177522718\n",
      "Epoch 8, Batch 100, Loss: 0.1321201810054481\n",
      "Epoch 8, Batch 200, Loss: 0.13508233247324825\n",
      "Epoch 8, Batch 300, Loss: 0.14539804559201003\n",
      "Epoch 8, Batch 400, Loss: 0.13133992506191133\n",
      "Epoch 8, Batch 500, Loss: 0.13736481249332427\n",
      "Epoch 8, Batch 600, Loss: 0.1276988115720451\n",
      "Epoch 9, Batch 100, Loss: 0.10778141624294221\n",
      "Epoch 9, Batch 200, Loss: 0.11752277032472193\n",
      "Epoch 9, Batch 300, Loss: 0.12387726026587188\n",
      "Epoch 9, Batch 400, Loss: 0.11605916288681328\n",
      "Epoch 9, Batch 500, Loss: 0.13213516785763205\n",
      "Epoch 9, Batch 600, Loss: 0.12335234481841326\n",
      "Epoch 10, Batch 100, Loss: 0.09705974020063877\n",
      "Epoch 10, Batch 200, Loss: 0.23155134425498544\n",
      "Epoch 10, Batch 300, Loss: 0.1597088176757097\n",
      "Epoch 10, Batch 400, Loss: 0.12958943728357553\n",
      "Epoch 10, Batch 500, Loss: 0.13905733949504792\n",
      "Epoch 10, Batch 600, Loss: 0.1279694485012442\n",
      "Epoch 11, Batch 100, Loss: 0.10834537324495613\n",
      "Epoch 11, Batch 200, Loss: 0.13180820466950535\n",
      "Epoch 11, Batch 300, Loss: 0.11320110874250532\n",
      "Epoch 11, Batch 400, Loss: 0.11510416144505144\n",
      "Epoch 11, Batch 500, Loss: 0.10143609480001033\n",
      "Epoch 11, Batch 600, Loss: 0.10784150866791606\n",
      "Epoch 12, Batch 100, Loss: 0.09259724704548716\n",
      "Epoch 12, Batch 200, Loss: 0.09605298658832907\n",
      "Epoch 12, Batch 300, Loss: 0.09708196490071713\n",
      "Epoch 12, Batch 400, Loss: 0.08228533078916371\n",
      "Epoch 12, Batch 500, Loss: 0.10557400556281209\n",
      "Epoch 12, Batch 600, Loss: 0.0916119662579149\n",
      "Epoch 13, Batch 100, Loss: 0.089572520153597\n",
      "Epoch 13, Batch 200, Loss: 0.07696203453699127\n",
      "Epoch 13, Batch 300, Loss: 0.08466636325698346\n",
      "Epoch 13, Batch 400, Loss: 0.08473032659851015\n",
      "Epoch 13, Batch 500, Loss: 0.08368764797691255\n",
      "Epoch 13, Batch 600, Loss: 0.0845351348631084\n",
      "Epoch 14, Batch 100, Loss: 0.06184275860898197\n",
      "Epoch 14, Batch 200, Loss: 0.06737640488892793\n",
      "Epoch 14, Batch 300, Loss: 0.06589907387504354\n",
      "Epoch 14, Batch 400, Loss: 0.07813626473769546\n",
      "Epoch 14, Batch 500, Loss: 0.06940793241839856\n",
      "Epoch 14, Batch 600, Loss: 0.08674944836180658\n",
      "Epoch 15, Batch 100, Loss: 0.05164467066060752\n",
      "Epoch 15, Batch 200, Loss: 0.05548516923794523\n",
      "Epoch 15, Batch 300, Loss: 0.05335938483360223\n",
      "Epoch 15, Batch 400, Loss: 0.061167305810377\n",
      "Epoch 15, Batch 500, Loss: 0.06048495274502784\n",
      "Epoch 15, Batch 600, Loss: 0.06257158081512898\n",
      "Epoch 16, Batch 100, Loss: 0.038007756187580524\n",
      "Epoch 16, Batch 200, Loss: 0.046392673240043224\n",
      "Epoch 16, Batch 300, Loss: 0.04331336650298909\n",
      "Epoch 16, Batch 400, Loss: 0.04315865628886968\n",
      "Epoch 16, Batch 500, Loss: 0.05295325516723096\n",
      "Epoch 16, Batch 600, Loss: 0.047658773720031605\n",
      "Epoch 17, Batch 100, Loss: 0.03805297960527241\n",
      "Epoch 17, Batch 200, Loss: 0.035409629310597664\n",
      "Epoch 17, Batch 300, Loss: 0.028036185406381264\n",
      "Epoch 17, Batch 400, Loss: 0.043901839372701944\n",
      "Epoch 17, Batch 500, Loss: 0.04047362800454721\n",
      "Epoch 17, Batch 600, Loss: 0.04038666971726343\n",
      "Epoch 18, Batch 100, Loss: 0.03209841377334669\n",
      "Epoch 18, Batch 200, Loss: 0.03081899250857532\n",
      "Epoch 18, Batch 300, Loss: 0.03090741422958672\n",
      "Epoch 18, Batch 400, Loss: 0.030076327896094882\n",
      "Epoch 18, Batch 500, Loss: 0.046611815579235556\n",
      "Epoch 18, Batch 600, Loss: 0.030519462022930384\n",
      "Epoch 19, Batch 100, Loss: 0.02736736122809816\n",
      "Epoch 19, Batch 200, Loss: 0.026948756257770583\n",
      "Epoch 19, Batch 300, Loss: 0.029416028111299966\n",
      "Epoch 19, Batch 400, Loss: 0.029590220761019737\n",
      "Epoch 19, Batch 500, Loss: 0.030663479587528856\n",
      "Epoch 19, Batch 600, Loss: 0.02570154283690499\n",
      "Epoch 20, Batch 100, Loss: 0.018164343857788482\n",
      "Epoch 20, Batch 200, Loss: 0.02128949975653086\n",
      "Epoch 20, Batch 300, Loss: 0.027676423206721666\n",
      "Epoch 20, Batch 400, Loss: 0.01782065859762952\n",
      "Epoch 20, Batch 500, Loss: 0.02347861934802495\n",
      "Epoch 20, Batch 600, Loss: 0.02510119399987161\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(20):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model._save_to_state_dict, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class UnlabeledTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [\n",
    "            os.path.join(root_dir, f) \n",
    "            for f in os.listdir(root_dir) \n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, os.path.basename(img_path)  # Return image + filename\n",
    "\n",
    "# Use validation transforms (no augmentation)\n",
    "test_dataset = UnlabeledTestDataset(\n",
    "    root_dir='test_images',\n",
    "    transform=val_test_transform  # From previous code\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "# Load the best saved model\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Test loop\n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, batch_filenames in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        filenames.extend(batch_filenames)\n",
    "\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "submission_df = pd.DataFrame({'img_name': os.listdir(test_dir), 'prediction': predictions})\n",
    "submission_df.to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Load training data\n",
    "train_dir = 'train_images'\n",
    "train_image_paths_0 = [os.path.join(train_dir, '0', file) for file in os.listdir(os.path.join(train_dir, '0'))]\n",
    "train_image_paths_1 = [os.path.join(train_dir, '1', file) for file in os.listdir(os.path.join(train_dir, '1'))]\n",
    "train_image_paths = train_image_paths_0 + train_image_paths_1\n",
    "train_labels = [0]*len(train_image_paths_0) + [1]*len(train_image_paths_1)\n",
    "\n",
    "# Load test data\n",
    "test_dir = 'test_images'\n",
    "test_image_paths = [os.path.join(test_dir, file) for file in os.listdir(test_dir)]\n",
    "test_labels = [0]*len(test_image_paths)  # Dummy labels for test set\n",
    "\n",
    "# Define data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.RandomHorizontalFlip(),  # Flip horizontally\n",
    "    transforms.RandomRotation(30),  # Rotate up to 30 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Adjust brightness, contrast, saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
    "])\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = CustomDataset(train_image_paths, train_labels, transform=transform)\n",
    "test_dataset = CustomDataset(test_image_paths, test_labels, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.6889347892999649\n",
      "Epoch 1, Batch 200, Loss: 0.6806009197235108\n",
      "Epoch 1, Batch 300, Loss: 0.6665885490179062\n",
      "Epoch 1, Batch 400, Loss: 0.6533500301837921\n",
      "Epoch 1, Batch 500, Loss: 0.648356072306633\n",
      "Epoch 1, Batch 600, Loss: 0.6315322688221932\n",
      "Epoch 1, Batch 700, Loss: 0.6344472920894623\n",
      "Epoch 1, Batch 800, Loss: 0.630073545575142\n",
      "Epoch 2, Batch 100, Loss: 0.6146052217483521\n",
      "Epoch 2, Batch 200, Loss: 0.5864030507206917\n",
      "Epoch 2, Batch 300, Loss: 0.5861467915773392\n",
      "Epoch 2, Batch 400, Loss: 0.5605809128284455\n",
      "Epoch 2, Batch 500, Loss: 0.5435501608252525\n",
      "Epoch 2, Batch 600, Loss: 0.49788765370845794\n",
      "Epoch 2, Batch 700, Loss: 0.4417063404619694\n",
      "Epoch 2, Batch 800, Loss: 0.37341064870357515\n",
      "Epoch 3, Batch 100, Loss: 0.32830569058656695\n",
      "Epoch 3, Batch 200, Loss: 0.31809785552322867\n",
      "Epoch 3, Batch 300, Loss: 0.2801007092744112\n",
      "Epoch 3, Batch 400, Loss: 0.2978315446525812\n",
      "Epoch 3, Batch 500, Loss: 0.2895161090046167\n",
      "Epoch 3, Batch 600, Loss: 0.2665874893218279\n",
      "Epoch 3, Batch 700, Loss: 0.2344049109145999\n",
      "Epoch 3, Batch 800, Loss: 0.2799659904837608\n",
      "Epoch 4, Batch 100, Loss: 0.4330963422358036\n",
      "Epoch 4, Batch 200, Loss: 0.26227033875882627\n",
      "Epoch 4, Batch 300, Loss: 0.26573784187436106\n",
      "Epoch 4, Batch 400, Loss: 0.22506371580064297\n",
      "Epoch 4, Batch 500, Loss: 0.23986063234508037\n",
      "Epoch 4, Batch 600, Loss: 0.21810157019644977\n",
      "Epoch 4, Batch 700, Loss: 0.23571380380541085\n",
      "Epoch 4, Batch 800, Loss: 0.2254425731115043\n",
      "Epoch 5, Batch 100, Loss: 0.23603025443851947\n",
      "Epoch 5, Batch 200, Loss: 0.21492614448070527\n",
      "Epoch 5, Batch 300, Loss: 0.2158704860135913\n",
      "Epoch 5, Batch 400, Loss: 0.21285679668188096\n",
      "Epoch 5, Batch 500, Loss: 0.23647050857543944\n",
      "Epoch 5, Batch 600, Loss: 0.18763821721076965\n",
      "Epoch 5, Batch 700, Loss: 0.20638378746807576\n",
      "Epoch 5, Batch 800, Loss: 0.22485654979944228\n",
      "Epoch 6, Batch 100, Loss: 0.21533727129921318\n",
      "Epoch 6, Batch 200, Loss: 0.21468732327222825\n",
      "Epoch 6, Batch 300, Loss: 0.21685384768992663\n",
      "Epoch 6, Batch 400, Loss: 0.30381643258035185\n",
      "Epoch 6, Batch 500, Loss: 0.22799547262489794\n",
      "Epoch 6, Batch 600, Loss: 0.1913943562656641\n",
      "Epoch 6, Batch 700, Loss: 0.192849972974509\n",
      "Epoch 6, Batch 800, Loss: 0.20708912124857307\n",
      "Epoch 7, Batch 100, Loss: 0.19263406835496424\n",
      "Epoch 7, Batch 200, Loss: 0.1885189814865589\n",
      "Epoch 7, Batch 300, Loss: 0.18719623152166606\n",
      "Epoch 7, Batch 400, Loss: 0.20087846875190735\n",
      "Epoch 7, Batch 500, Loss: 0.18664307249709963\n",
      "Epoch 7, Batch 600, Loss: 0.2030521073937416\n",
      "Epoch 7, Batch 700, Loss: 0.21597425152547658\n",
      "Epoch 7, Batch 800, Loss: 0.19719686456024646\n",
      "Epoch 8, Batch 100, Loss: 0.20143402323126794\n",
      "Epoch 8, Batch 200, Loss: 0.18983392586931586\n",
      "Epoch 8, Batch 300, Loss: 0.1953754173219204\n",
      "Epoch 8, Batch 400, Loss: 0.18012757757678627\n",
      "Epoch 8, Batch 500, Loss: 0.19066051898524164\n",
      "Epoch 8, Batch 600, Loss: 0.19858771078288556\n",
      "Epoch 8, Batch 700, Loss: 0.1932970749028027\n",
      "Epoch 8, Batch 800, Loss: 0.18086904514580965\n",
      "Epoch 9, Batch 100, Loss: 0.17731437996029853\n",
      "Epoch 9, Batch 200, Loss: 0.18821128975600004\n",
      "Epoch 9, Batch 300, Loss: 0.18822684317827224\n",
      "Epoch 9, Batch 400, Loss: 0.19549879170954226\n",
      "Epoch 9, Batch 500, Loss: 0.19291979551315308\n",
      "Epoch 9, Batch 600, Loss: 0.22574785320088267\n",
      "Epoch 9, Batch 700, Loss: 0.21382546778768302\n",
      "Epoch 9, Batch 800, Loss: 0.20126961838454008\n",
      "Epoch 10, Batch 100, Loss: 0.18851407933980227\n",
      "Epoch 10, Batch 200, Loss: 0.18252162508666514\n",
      "Epoch 10, Batch 300, Loss: 0.18257750120013952\n",
      "Epoch 10, Batch 400, Loss: 0.18483668085187674\n",
      "Epoch 10, Batch 500, Loss: 0.17280460942536593\n",
      "Epoch 10, Batch 600, Loss: 0.2356976966187358\n",
      "Epoch 10, Batch 700, Loss: 0.2007141694240272\n",
      "Epoch 10, Batch 800, Loss: 0.17352632883936167\n",
      "Epoch 11, Batch 100, Loss: 0.18279382899403573\n",
      "Epoch 11, Batch 200, Loss: 0.1872420810163021\n",
      "Epoch 11, Batch 300, Loss: 0.18127037614583968\n",
      "Epoch 11, Batch 400, Loss: 0.1855591114796698\n",
      "Epoch 11, Batch 500, Loss: 0.17383148219436406\n",
      "Epoch 11, Batch 600, Loss: 0.17556569498032332\n",
      "Epoch 11, Batch 700, Loss: 0.18012864831835032\n",
      "Epoch 11, Batch 800, Loss: 0.15782646249979734\n",
      "Epoch 12, Batch 100, Loss: 0.19942483879625797\n",
      "Epoch 12, Batch 200, Loss: 0.18824106810614466\n",
      "Epoch 12, Batch 300, Loss: 0.1757113314792514\n",
      "Epoch 12, Batch 400, Loss: 0.17714352129027247\n",
      "Epoch 12, Batch 500, Loss: 0.18289811424911023\n",
      "Epoch 12, Batch 600, Loss: 0.1611853426322341\n",
      "Epoch 12, Batch 700, Loss: 0.18084957636892796\n",
      "Epoch 12, Batch 800, Loss: 0.17459690384566784\n",
      "Epoch 13, Batch 100, Loss: 0.18147166680544616\n",
      "Epoch 13, Batch 200, Loss: 0.16974731108173727\n",
      "Epoch 13, Batch 300, Loss: 0.17960587283596396\n",
      "Epoch 13, Batch 400, Loss: 0.1984815826267004\n",
      "Epoch 13, Batch 500, Loss: 0.16856446368619799\n",
      "Epoch 13, Batch 600, Loss: 0.18368141911923885\n",
      "Epoch 13, Batch 700, Loss: 0.16906719300895928\n",
      "Epoch 13, Batch 800, Loss: 0.1730579208023846\n",
      "Epoch 14, Batch 100, Loss: 0.16005752326920628\n",
      "Epoch 14, Batch 200, Loss: 0.18201162297278642\n",
      "Epoch 14, Batch 300, Loss: 0.1576103071309626\n",
      "Epoch 14, Batch 400, Loss: 0.17697387371212245\n",
      "Epoch 14, Batch 500, Loss: 0.17025409422814847\n",
      "Epoch 14, Batch 600, Loss: 0.18703933794051408\n",
      "Epoch 14, Batch 700, Loss: 0.16101541748270393\n",
      "Epoch 14, Batch 800, Loss: 0.17061594139784575\n",
      "Epoch 15, Batch 100, Loss: 0.15436868108808993\n",
      "Epoch 15, Batch 200, Loss: 0.18308320822194218\n",
      "Epoch 15, Batch 300, Loss: 0.16243932753801346\n",
      "Epoch 15, Batch 400, Loss: 0.17490150140598415\n",
      "Epoch 15, Batch 500, Loss: 0.19140420027077198\n",
      "Epoch 15, Batch 600, Loss: 0.18767780844122173\n",
      "Epoch 15, Batch 700, Loss: 0.158713919557631\n",
      "Epoch 15, Batch 800, Loss: 0.17325114937499164\n",
      "Epoch 16, Batch 100, Loss: 0.1746105633303523\n",
      "Epoch 16, Batch 200, Loss: 0.16966944685205818\n",
      "Epoch 16, Batch 300, Loss: 0.1781926181912422\n",
      "Epoch 16, Batch 400, Loss: 0.17858134340494872\n",
      "Epoch 16, Batch 500, Loss: 0.18304667072370648\n",
      "Epoch 16, Batch 600, Loss: 0.16868680652230977\n",
      "Epoch 16, Batch 700, Loss: 0.17913972929120064\n",
      "Epoch 16, Batch 800, Loss: 0.1622977178171277\n",
      "Epoch 17, Batch 100, Loss: 0.16689097685739399\n",
      "Epoch 17, Batch 200, Loss: 0.1613996409252286\n",
      "Epoch 17, Batch 300, Loss: 0.16406937208026648\n",
      "Epoch 17, Batch 400, Loss: 0.1618318670243025\n",
      "Epoch 17, Batch 500, Loss: 0.19629426011815668\n",
      "Epoch 17, Batch 600, Loss: 0.16806188866496086\n",
      "Epoch 17, Batch 700, Loss: 0.1708317076601088\n",
      "Epoch 17, Batch 800, Loss: 0.17351538760587573\n",
      "Epoch 18, Batch 100, Loss: 0.1926420847326517\n",
      "Epoch 18, Batch 200, Loss: 0.16102138765156268\n",
      "Epoch 18, Batch 300, Loss: 0.16408101815730333\n",
      "Epoch 18, Batch 400, Loss: 0.15571553472429514\n",
      "Epoch 18, Batch 500, Loss: 0.1595474779419601\n",
      "Epoch 18, Batch 600, Loss: 0.15149906370788813\n",
      "Epoch 18, Batch 700, Loss: 0.16298293866217137\n",
      "Epoch 18, Batch 800, Loss: 0.16101749952882527\n",
      "Epoch 19, Batch 100, Loss: 0.15987066335976124\n",
      "Epoch 19, Batch 200, Loss: 0.14282644201070072\n",
      "Epoch 19, Batch 300, Loss: 0.1634482266381383\n",
      "Epoch 19, Batch 400, Loss: 0.16969905341044067\n",
      "Epoch 19, Batch 500, Loss: 0.15726541882380843\n",
      "Epoch 19, Batch 600, Loss: 0.18162621458992362\n",
      "Epoch 19, Batch 700, Loss: 0.15788430375978352\n",
      "Epoch 19, Batch 800, Loss: 0.17961537091061472\n",
      "Epoch 20, Batch 100, Loss: 0.16921073211356996\n",
      "Epoch 20, Batch 200, Loss: 0.1552680345065892\n",
      "Epoch 20, Batch 300, Loss: 0.14849746078252793\n",
      "Epoch 20, Batch 400, Loss: 0.16799924148246645\n",
      "Epoch 20, Batch 500, Loss: 0.16813158929347993\n",
      "Epoch 20, Batch 600, Loss: 0.15671060377731919\n",
      "Epoch 20, Batch 700, Loss: 0.16033163499087094\n",
      "Epoch 20, Batch 800, Loss: 0.16725273801013826\n",
      "Epoch 21, Batch 100, Loss: 0.1658682486973703\n",
      "Epoch 21, Batch 200, Loss: 0.14913328491151334\n",
      "Epoch 21, Batch 300, Loss: 0.15836909348145128\n",
      "Epoch 21, Batch 400, Loss: 0.16384644575417043\n",
      "Epoch 21, Batch 500, Loss: 0.16820049209520221\n",
      "Epoch 21, Batch 600, Loss: 0.16464444929733874\n",
      "Epoch 21, Batch 700, Loss: 0.17010412899777294\n",
      "Epoch 21, Batch 800, Loss: 0.16455859422683716\n",
      "Epoch 22, Batch 100, Loss: 0.16398186210542917\n",
      "Epoch 22, Batch 200, Loss: 0.1624332388676703\n",
      "Epoch 22, Batch 300, Loss: 0.1769140024483204\n",
      "Epoch 22, Batch 400, Loss: 0.15773308739066125\n",
      "Epoch 22, Batch 500, Loss: 0.17395261416211724\n",
      "Epoch 22, Batch 600, Loss: 0.1739138384349644\n",
      "Epoch 22, Batch 700, Loss: 0.1619095044583082\n",
      "Epoch 22, Batch 800, Loss: 0.16684701800346374\n",
      "Epoch 23, Batch 100, Loss: 0.14898399336263538\n",
      "Epoch 23, Batch 200, Loss: 0.15989790596067904\n",
      "Epoch 23, Batch 300, Loss: 0.1612512570247054\n",
      "Epoch 23, Batch 400, Loss: 0.16737132063135504\n",
      "Epoch 23, Batch 500, Loss: 0.16768025152385235\n",
      "Epoch 23, Batch 600, Loss: 0.18161796718835832\n",
      "Epoch 23, Batch 700, Loss: 0.15808006312698125\n",
      "Epoch 23, Batch 800, Loss: 0.16291347194463016\n",
      "Epoch 24, Batch 100, Loss: 0.17480827152729034\n",
      "Epoch 24, Batch 200, Loss: 0.1615343464165926\n",
      "Epoch 24, Batch 300, Loss: 0.17199179742485285\n",
      "Epoch 24, Batch 400, Loss: 0.16556507440283894\n",
      "Epoch 24, Batch 500, Loss: 0.16793799182400107\n",
      "Epoch 24, Batch 600, Loss: 0.15798443539068102\n",
      "Epoch 24, Batch 700, Loss: 0.17652125969529153\n",
      "Epoch 24, Batch 800, Loss: 0.1662441634014249\n",
      "Epoch 25, Batch 100, Loss: 0.1583981930464506\n",
      "Epoch 25, Batch 200, Loss: 0.17469018656760454\n",
      "Epoch 25, Batch 300, Loss: 0.15785911297425628\n",
      "Epoch 25, Batch 400, Loss: 0.17191567417234183\n",
      "Epoch 25, Batch 500, Loss: 0.15537426443770527\n",
      "Epoch 25, Batch 600, Loss: 0.16533523064106703\n",
      "Epoch 25, Batch 700, Loss: 0.1669988911226392\n",
      "Epoch 25, Batch 800, Loss: 0.16732650680467487\n",
      "Epoch 26, Batch 100, Loss: 0.15856307396665215\n",
      "Epoch 26, Batch 200, Loss: 0.16380520015954972\n",
      "Epoch 26, Batch 300, Loss: 0.14577946793287994\n",
      "Epoch 26, Batch 400, Loss: 0.1726758912950754\n",
      "Epoch 26, Batch 500, Loss: 0.15941555494442583\n",
      "Epoch 26, Batch 600, Loss: 0.17136124128475785\n",
      "Epoch 26, Batch 700, Loss: 0.1569313656911254\n",
      "Epoch 26, Batch 800, Loss: 0.27713752288371324\n",
      "Epoch 27, Batch 100, Loss: 0.14638266365975142\n",
      "Epoch 27, Batch 200, Loss: 0.1634039886854589\n",
      "Epoch 27, Batch 300, Loss: 0.18413272637873887\n",
      "Epoch 27, Batch 400, Loss: 0.16443945948034525\n",
      "Epoch 27, Batch 500, Loss: 0.17921406637877227\n",
      "Epoch 27, Batch 600, Loss: 0.1477986167371273\n",
      "Epoch 27, Batch 700, Loss: 0.15314454277977346\n",
      "Epoch 27, Batch 800, Loss: 0.16461937008425592\n",
      "Epoch 28, Batch 100, Loss: 0.15910798272117974\n",
      "Epoch 28, Batch 200, Loss: 0.1676827460899949\n",
      "Epoch 28, Batch 300, Loss: 0.16463880106806755\n",
      "Epoch 28, Batch 400, Loss: 0.15395493355579673\n",
      "Epoch 28, Batch 500, Loss: 0.16346692500635981\n",
      "Epoch 28, Batch 600, Loss: 0.17014689315110446\n",
      "Epoch 28, Batch 700, Loss: 0.16581197671592235\n",
      "Epoch 28, Batch 800, Loss: 0.15383904004469515\n",
      "Epoch 29, Batch 100, Loss: 0.1520680484175682\n",
      "Epoch 29, Batch 200, Loss: 0.15544312998652457\n",
      "Epoch 29, Batch 300, Loss: 0.16415347095578908\n",
      "Epoch 29, Batch 400, Loss: 0.16879920963197947\n",
      "Epoch 29, Batch 500, Loss: 0.14822462026029826\n",
      "Epoch 29, Batch 600, Loss: 0.15941847631707787\n",
      "Epoch 29, Batch 700, Loss: 0.17806877203285695\n",
      "Epoch 29, Batch 800, Loss: 0.1718725556693971\n",
      "Epoch 30, Batch 100, Loss: 0.16087485725060105\n",
      "Epoch 30, Batch 200, Loss: 0.16553670562803746\n",
      "Epoch 30, Batch 300, Loss: 0.16723030120134352\n",
      "Epoch 30, Batch 400, Loss: 0.14482081770896912\n",
      "Epoch 30, Batch 500, Loss: 0.15145667005330324\n",
      "Epoch 30, Batch 600, Loss: 0.15998473888263107\n",
      "Epoch 30, Batch 700, Loss: 0.15635475512593985\n",
      "Epoch 30, Batch 800, Loss: 0.1532114435173571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a model with dropout and different activation functions\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.AvgPool2d(2, 2)  # Use average pooling\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "        self.activation = nn.LeakyReLU()  # Use LeakyReLU activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer with L2 regularization\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)  # L2 regularization\n",
    "model = model.to(device)\n",
    "# Define a function to calculate accuracy\n",
    "def calculate_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Initialize lists to track metrics\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "patience = 5  # Number of epochs to wait before stopping\n",
    "best_test_loss = float('inf')  # Initialize best test loss\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Train the model\n",
    "# Train the model\n",
    "for epoch in range(30):  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m criterion = nn.CrossEntropyLoss()  \u001b[38;5;66;03m# Instead of nn.BCELoss()\u001b[39;00m\n\u001b[32m     30\u001b[39m optimizer = optim.SGD(model.parameters(), lr=\u001b[32m0.001\u001b[39m, momentum=\u001b[32m0.9\u001b[39m, weight_decay=\u001b[32m0.01\u001b[39m)  \u001b[38;5;66;03m# L2 regularization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m model = model.to(\u001b[43mdevice\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Correct in_channels to 3 for RGB input\n",
    "        self.conv = nn.Conv2d(3, 32, 3)  # Changed from 6 to 3\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 26 * 26, 512)\n",
    "        self.fc2 = nn.Linear(512, 300)\n",
    "        self.fc3 = nn.Linear(300, 120)\n",
    "        self.fc4 = nn.Linear(120, 2)\n",
    "        self.activation = nn.LeakyReLU()  # Use LeakyReLU activation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 26 * 26)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "model = Net()\n",
    "# Replace BCELoss with CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()  # Instead of nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.01)  # L2 regularization\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 100, Loss: 0.6927591812610626\n",
      "Epoch 1, Batch 200, Loss: 0.6915788787603379\n",
      "Epoch 1, Batch 300, Loss: 0.6909683918952942\n",
      "Epoch 1, Batch 400, Loss: 0.6885539555549621\n",
      "Epoch 1, Batch 500, Loss: 0.6868093043565751\n",
      "Epoch 1, Batch 600, Loss: 0.6828656160831451\n",
      "Epoch 1, Batch 700, Loss: 0.6756742423772812\n",
      "Epoch 1, Batch 800, Loss: 0.6688040602207184\n",
      "Epoch 2, Batch 100, Loss: 0.659545236825943\n",
      "Epoch 2, Batch 200, Loss: 0.6558779150247573\n",
      "Epoch 2, Batch 300, Loss: 0.6558793145418167\n",
      "Epoch 2, Batch 400, Loss: 0.661699030995369\n",
      "Epoch 2, Batch 500, Loss: 0.6406351405382157\n",
      "Epoch 2, Batch 600, Loss: 0.6405650895833969\n",
      "Epoch 2, Batch 700, Loss: 0.6387910962104797\n",
      "Epoch 2, Batch 800, Loss: 0.6316255408525467\n",
      "Epoch 3, Batch 100, Loss: 0.6254412907361985\n",
      "Epoch 3, Batch 200, Loss: 0.619899353981018\n",
      "Epoch 3, Batch 300, Loss: 0.6170816588401794\n",
      "Epoch 3, Batch 400, Loss: 0.610457630455494\n",
      "Epoch 3, Batch 500, Loss: 0.6095782920718193\n",
      "Epoch 3, Batch 600, Loss: 0.6006646713614464\n",
      "Epoch 3, Batch 700, Loss: 0.6065300852060318\n",
      "Epoch 3, Batch 800, Loss: 0.5851768809556961\n",
      "Epoch 4, Batch 100, Loss: 0.5863786509633064\n",
      "Epoch 4, Batch 200, Loss: 0.57078365534544\n",
      "Epoch 4, Batch 300, Loss: 0.561586177945137\n",
      "Epoch 4, Batch 400, Loss: 0.5532949638366699\n",
      "Epoch 4, Batch 500, Loss: 0.5181020349264145\n",
      "Epoch 4, Batch 600, Loss: 0.5066420486569405\n",
      "Epoch 4, Batch 700, Loss: 0.48806928753852846\n",
      "Epoch 4, Batch 800, Loss: 0.4690908798575401\n",
      "Epoch 5, Batch 100, Loss: 0.4471457481384277\n",
      "Epoch 5, Batch 200, Loss: 0.4388484367728233\n",
      "Epoch 5, Batch 300, Loss: 0.4368657621741295\n",
      "Epoch 5, Batch 400, Loss: 0.41607928842306136\n",
      "Epoch 5, Batch 500, Loss: 0.4251033639907837\n",
      "Epoch 5, Batch 600, Loss: 0.41773249477148056\n",
      "Epoch 5, Batch 700, Loss: 0.4130043092370033\n",
      "Epoch 5, Batch 800, Loss: 0.4198218062520027\n",
      "Epoch 6, Batch 100, Loss: 0.4045900869369507\n",
      "Epoch 6, Batch 200, Loss: 0.4034968030452728\n",
      "Epoch 6, Batch 300, Loss: 0.40151416838169096\n",
      "Epoch 6, Batch 400, Loss: 0.4086372840404511\n",
      "Epoch 6, Batch 500, Loss: 0.4051911374926567\n",
      "Epoch 6, Batch 600, Loss: 0.40994065314531325\n",
      "Epoch 6, Batch 700, Loss: 0.4072378194332123\n",
      "Epoch 6, Batch 800, Loss: 0.40299612253904343\n",
      "Epoch 7, Batch 100, Loss: 0.40008114129304884\n",
      "Epoch 7, Batch 200, Loss: 0.40647553503513334\n",
      "Epoch 7, Batch 300, Loss: 0.39759875416755674\n",
      "Epoch 7, Batch 400, Loss: 0.39955966889858247\n",
      "Epoch 7, Batch 500, Loss: 0.40125912249088286\n",
      "Epoch 7, Batch 600, Loss: 0.4081946983933449\n",
      "Epoch 7, Batch 700, Loss: 0.4021656411886215\n",
      "Epoch 7, Batch 800, Loss: 0.38735441654920577\n",
      "Epoch 8, Batch 100, Loss: 0.4033812952041626\n",
      "Epoch 8, Batch 200, Loss: 0.39464789927005767\n",
      "Epoch 8, Batch 300, Loss: 0.3943173363804817\n",
      "Epoch 8, Batch 400, Loss: 0.4048240980505943\n",
      "Epoch 8, Batch 500, Loss: 0.3912047106027603\n",
      "Epoch 8, Batch 600, Loss: 0.3990650898218155\n",
      "Epoch 8, Batch 700, Loss: 0.39590158075094223\n",
      "Epoch 8, Batch 800, Loss: 0.4010714042186737\n",
      "Epoch 9, Batch 100, Loss: 0.40317285269498826\n",
      "Epoch 9, Batch 200, Loss: 0.40251467168331145\n",
      "Epoch 9, Batch 300, Loss: 0.39990593314170836\n",
      "Epoch 9, Batch 400, Loss: 0.39326710879802707\n",
      "Epoch 9, Batch 500, Loss: 0.3865570583939552\n",
      "Epoch 9, Batch 600, Loss: 0.3820618784427643\n",
      "Epoch 9, Batch 700, Loss: 0.4008913969993591\n",
      "Epoch 9, Batch 800, Loss: 0.3990066847205162\n",
      "Epoch 10, Batch 100, Loss: 0.3979925560951233\n",
      "Epoch 10, Batch 200, Loss: 0.3967993199825287\n",
      "Epoch 10, Batch 300, Loss: 0.3848400899767876\n",
      "Epoch 10, Batch 400, Loss: 0.39722910910844805\n",
      "Epoch 10, Batch 500, Loss: 0.3912038865685463\n",
      "Epoch 10, Batch 600, Loss: 0.3975234159827232\n",
      "Epoch 10, Batch 700, Loss: 0.3981193232536316\n",
      "Epoch 10, Batch 800, Loss: 0.39431548535823824\n",
      "Epoch 11, Batch 100, Loss: 0.4004939886927605\n",
      "Epoch 11, Batch 200, Loss: 0.3974331170320511\n",
      "Epoch 11, Batch 300, Loss: 0.3963659405708313\n",
      "Epoch 11, Batch 400, Loss: 0.39143155694007875\n",
      "Epoch 11, Batch 500, Loss: 0.3866188368201256\n",
      "Epoch 11, Batch 600, Loss: 0.39957911491394044\n",
      "Epoch 11, Batch 700, Loss: 0.3948710432648659\n",
      "Epoch 11, Batch 800, Loss: 0.3945218887925148\n",
      "Epoch 12, Batch 100, Loss: 0.3938749188184738\n",
      "Epoch 12, Batch 200, Loss: 0.3948193952441216\n",
      "Epoch 12, Batch 300, Loss: 0.3916303452849388\n",
      "Epoch 12, Batch 400, Loss: 0.4003473198413849\n",
      "Epoch 12, Batch 500, Loss: 0.39401861488819123\n",
      "Epoch 12, Batch 600, Loss: 0.39052278220653536\n",
      "Epoch 12, Batch 700, Loss: 0.3906583571434021\n",
      "Epoch 12, Batch 800, Loss: 0.3982045978307724\n",
      "Epoch 13, Batch 100, Loss: 0.3990337285399437\n",
      "Epoch 13, Batch 200, Loss: 0.3918459838628769\n",
      "Epoch 13, Batch 300, Loss: 0.39473421752452853\n",
      "Epoch 13, Batch 400, Loss: 0.38401039034128187\n",
      "Epoch 13, Batch 500, Loss: 0.3961800757050514\n",
      "Epoch 13, Batch 600, Loss: 0.38837609350681307\n",
      "Epoch 13, Batch 700, Loss: 0.3962944424152374\n",
      "Epoch 13, Batch 800, Loss: 0.3977531635761261\n",
      "Epoch 14, Batch 100, Loss: 0.3917823112010956\n",
      "Epoch 14, Batch 200, Loss: 0.4009671911597252\n",
      "Epoch 14, Batch 300, Loss: 0.4015022674202919\n",
      "Epoch 14, Batch 400, Loss: 0.3866968995332718\n",
      "Epoch 14, Batch 500, Loss: 0.39244495987892153\n",
      "Epoch 14, Batch 600, Loss: 0.3794838199019432\n",
      "Epoch 14, Batch 700, Loss: 0.39232021480798723\n",
      "Epoch 14, Batch 800, Loss: 0.39109949409961703\n",
      "Epoch 15, Batch 100, Loss: 0.3946392786502838\n",
      "Epoch 15, Batch 200, Loss: 0.39054515928030015\n",
      "Epoch 15, Batch 300, Loss: 0.38741837978363036\n",
      "Epoch 15, Batch 400, Loss: 0.3910950458049774\n",
      "Epoch 15, Batch 500, Loss: 0.3936038142442703\n",
      "Epoch 15, Batch 600, Loss: 0.39499390572309495\n",
      "Epoch 15, Batch 700, Loss: 0.3914390483498573\n",
      "Epoch 15, Batch 800, Loss: 0.39226709127426146\n",
      "Epoch 16, Batch 100, Loss: 0.39003381848335267\n",
      "Epoch 16, Batch 200, Loss: 0.39653437346220016\n",
      "Epoch 16, Batch 300, Loss: 0.3934073281288147\n",
      "Epoch 16, Batch 400, Loss: 0.3933804246783257\n",
      "Epoch 16, Batch 500, Loss: 0.39547059416770936\n",
      "Epoch 16, Batch 600, Loss: 0.3930622413754463\n",
      "Epoch 16, Batch 700, Loss: 0.3881631836295128\n",
      "Epoch 16, Batch 800, Loss: 0.3951481860876083\n",
      "Epoch 17, Batch 100, Loss: 0.3940232136845589\n",
      "Epoch 17, Batch 200, Loss: 0.392219201028347\n",
      "Epoch 17, Batch 300, Loss: 0.39219283789396286\n",
      "Epoch 17, Batch 400, Loss: 0.401581244468689\n",
      "Epoch 17, Batch 500, Loss: 0.3980031126737595\n",
      "Epoch 17, Batch 600, Loss: 0.3851813617348671\n",
      "Epoch 17, Batch 700, Loss: 0.3954290524125099\n",
      "Epoch 17, Batch 800, Loss: 0.39431351870298387\n",
      "Epoch 18, Batch 100, Loss: 0.3962110671401024\n",
      "Epoch 18, Batch 200, Loss: 0.40138006895780565\n",
      "Epoch 18, Batch 300, Loss: 0.3871848690509796\n",
      "Epoch 18, Batch 400, Loss: 0.39399590849876404\n",
      "Epoch 18, Batch 500, Loss: 0.39997134953737257\n",
      "Epoch 18, Batch 600, Loss: 0.39221083104610444\n",
      "Epoch 18, Batch 700, Loss: 0.38987777173519134\n",
      "Epoch 18, Batch 800, Loss: 0.39059810698032377\n",
      "Epoch 19, Batch 100, Loss: 0.3927538961172104\n",
      "Epoch 19, Batch 200, Loss: 0.39265298664569853\n",
      "Epoch 19, Batch 300, Loss: 0.38841895401477816\n",
      "Epoch 19, Batch 400, Loss: 0.38985982924699786\n",
      "Epoch 19, Batch 500, Loss: 0.38996120482683183\n",
      "Epoch 19, Batch 600, Loss: 0.3985138216614723\n",
      "Epoch 19, Batch 700, Loss: 0.3885698741674423\n",
      "Epoch 19, Batch 800, Loss: 0.3990830335021019\n",
      "Epoch 20, Batch 100, Loss: 0.3994974601268768\n",
      "Epoch 20, Batch 200, Loss: 0.40356640458106996\n",
      "Epoch 20, Batch 300, Loss: 0.3884697118401527\n",
      "Epoch 20, Batch 400, Loss: 0.3924348765611649\n",
      "Epoch 20, Batch 500, Loss: 0.39060716271400453\n",
      "Epoch 20, Batch 600, Loss: 0.3946487411856651\n",
      "Epoch 20, Batch 700, Loss: 0.3916350618004799\n",
      "Epoch 20, Batch 800, Loss: 0.38765374064445496\n",
      "Epoch 21, Batch 100, Loss: 0.3935741093754768\n",
      "Epoch 21, Batch 200, Loss: 0.393121549487114\n",
      "Epoch 21, Batch 300, Loss: 0.3896001249551773\n",
      "Epoch 21, Batch 400, Loss: 0.38608085572719575\n",
      "Epoch 21, Batch 500, Loss: 0.39464371144771576\n",
      "Epoch 21, Batch 600, Loss: 0.3993830427527428\n",
      "Epoch 21, Batch 700, Loss: 0.39562196522951126\n",
      "Epoch 21, Batch 800, Loss: 0.39450034946203233\n",
      "Epoch 22, Batch 100, Loss: 0.3956111338734627\n",
      "Epoch 22, Batch 200, Loss: 0.38736614406108855\n",
      "Epoch 22, Batch 300, Loss: 0.38766254007816314\n",
      "Epoch 22, Batch 400, Loss: 0.3937817600369453\n",
      "Epoch 22, Batch 500, Loss: 0.3992397767305374\n",
      "Epoch 22, Batch 600, Loss: 0.39288733422756195\n",
      "Epoch 22, Batch 700, Loss: 0.3868913671374321\n",
      "Epoch 22, Batch 800, Loss: 0.3902782827615738\n",
      "Epoch 23, Batch 100, Loss: 0.38773723900318147\n",
      "Epoch 23, Batch 200, Loss: 0.3996385428309441\n",
      "Epoch 23, Batch 300, Loss: 0.3873870202898979\n",
      "Epoch 23, Batch 400, Loss: 0.39398847579956053\n",
      "Epoch 23, Batch 500, Loss: 0.38794540852308274\n",
      "Epoch 23, Batch 600, Loss: 0.40051629066467287\n",
      "Epoch 23, Batch 700, Loss: 0.3995689013600349\n",
      "Epoch 23, Batch 800, Loss: 0.38543454647064207\n",
      "Epoch 24, Batch 100, Loss: 0.40279868364334104\n",
      "Epoch 24, Batch 200, Loss: 0.40080418437719345\n",
      "Epoch 24, Batch 300, Loss: 0.384554044008255\n",
      "Epoch 24, Batch 400, Loss: 0.3968743786215782\n",
      "Epoch 24, Batch 500, Loss: 0.39367226243019104\n",
      "Epoch 24, Batch 600, Loss: 0.39045747488737104\n",
      "Epoch 24, Batch 700, Loss: 0.39072919577360155\n",
      "Epoch 24, Batch 800, Loss: 0.3819140848517418\n",
      "Epoch 25, Batch 100, Loss: 0.38729079097509383\n",
      "Epoch 25, Batch 200, Loss: 0.39149264484643936\n",
      "Epoch 25, Batch 300, Loss: 0.3861316895484924\n",
      "Epoch 25, Batch 400, Loss: 0.38439385503530504\n",
      "Epoch 25, Batch 500, Loss: 0.3983574792742729\n",
      "Epoch 25, Batch 600, Loss: 0.38715302765369414\n",
      "Epoch 25, Batch 700, Loss: 0.3964958897233009\n",
      "Epoch 25, Batch 800, Loss: 0.39721222817897794\n",
      "Epoch 26, Batch 100, Loss: 0.39220495998859406\n",
      "Epoch 26, Batch 200, Loss: 0.38898487329483034\n",
      "Epoch 26, Batch 300, Loss: 0.39209830194711687\n",
      "Epoch 26, Batch 400, Loss: 0.3849213057756424\n",
      "Epoch 26, Batch 500, Loss: 0.39133909314870835\n",
      "Epoch 26, Batch 600, Loss: 0.3932108974456787\n",
      "Epoch 26, Batch 700, Loss: 0.40957577288150787\n",
      "Epoch 26, Batch 800, Loss: 0.39180830627679825\n",
      "Epoch 27, Batch 100, Loss: 0.393179352581501\n",
      "Epoch 27, Batch 200, Loss: 0.39283105432987214\n",
      "Epoch 27, Batch 300, Loss: 0.3841708654165268\n",
      "Epoch 27, Batch 400, Loss: 0.38824144572019575\n",
      "Epoch 27, Batch 500, Loss: 0.39125500708818434\n",
      "Epoch 27, Batch 600, Loss: 0.39383976578712465\n",
      "Epoch 27, Batch 700, Loss: 0.3958994218707085\n",
      "Epoch 27, Batch 800, Loss: 0.38989855974912646\n",
      "Epoch 28, Batch 100, Loss: 0.3968505936861038\n",
      "Epoch 28, Batch 200, Loss: 0.3959136775135994\n",
      "Epoch 28, Batch 300, Loss: 0.3979071968793869\n",
      "Epoch 28, Batch 400, Loss: 0.39159516781568526\n",
      "Epoch 28, Batch 500, Loss: 0.39780511766672133\n",
      "Epoch 28, Batch 600, Loss: 0.38721943795681\n",
      "Epoch 28, Batch 700, Loss: 0.379816929101944\n",
      "Epoch 28, Batch 800, Loss: 0.39781876653432846\n",
      "Epoch 29, Batch 100, Loss: 0.39538202732801436\n",
      "Epoch 29, Batch 200, Loss: 0.38583937466144563\n",
      "Epoch 29, Batch 300, Loss: 0.38117643594741824\n",
      "Epoch 29, Batch 400, Loss: 0.3965208846330643\n",
      "Epoch 29, Batch 500, Loss: 0.4014570316672325\n",
      "Epoch 29, Batch 600, Loss: 0.4018320605158806\n",
      "Epoch 29, Batch 700, Loss: 0.3970879802107811\n",
      "Epoch 29, Batch 800, Loss: 0.39596384048461913\n",
      "Epoch 30, Batch 100, Loss: 0.39750108659267425\n",
      "Epoch 30, Batch 200, Loss: 0.3884395405650139\n",
      "Epoch 30, Batch 300, Loss: 0.38568608224391937\n",
      "Epoch 30, Batch 400, Loss: 0.3918873617053032\n",
      "Epoch 30, Batch 500, Loss: 0.3908772751688957\n",
      "Epoch 30, Batch 600, Loss: 0.3963532316684723\n",
      "Epoch 30, Batch 700, Loss: 0.3936996740102768\n",
      "Epoch 30, Batch 800, Loss: 0.39337204307317736\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Initialize lists to track metrics\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "# Initialize early stopping parameters\n",
    "patience = 5  # Number of epochs to wait before stopping\n",
    "best_test_loss = float('inf')  # Initialize best test loss\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Train the model\n",
    "# Train the model\n",
    "for epoch in range(30):  \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss / 100}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, _ = data\n",
    "        inputs = inputs.to(device)  # Move data to GPU\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission_df = pd.DataFrame({'img_name': os.listdir(test_dir), 'prediction': predictions})\n",
    "submission_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
